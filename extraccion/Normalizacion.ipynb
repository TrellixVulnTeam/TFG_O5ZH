{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e164ecd8",
   "metadata": {},
   "source": [
    "# Limpieza y Normalización de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be60dfd2",
   "metadata": {},
   "source": [
    "## Imports e Inicializaciones necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88562c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/juanmoreno/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/juanmoreno/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import operator\n",
    "import heapq\n",
    "import ast\n",
    "import seaborn as sns\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import wikipedia\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from sklearn import preprocessing\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "black_list = ['más', 'mas', 'unir', 'paises', 'pais', 'espa', 'no', 'os', 'a', 'compa', 'acompa', 'off', 'and', 'grecia', 'the','it', 'to',\n",
    "              'd',  'et',  'dame',  'il',  'dans', 'that',  'as',   'for',  'it',  'elections',  'would',  'this',  'with', 'york', 'obama', 'chavez', 'gadafi']\n",
    "\n",
    "stop = set(stopwords.words('spanish'))\n",
    "additional_stopwords=set(black_list)\n",
    "stopwords = stop.union(additional_stopwords)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "ner_dict = {0: 'PER', 1: 'MISC', 2: 'ORG', 3: 'UND'}\n",
    "regex = \"[a-zA-Z0-9]*_[a-zA-Z0-9]*\"\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "nlp.max_length = 3000000\n",
    "nlp2 = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44267ddd",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec0f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDuplicatesWithCount(listOfElems):\n",
    "    ''' Get frequency count of duplicate elements in the given list '''\n",
    "    dictOfElems = dict()\n",
    "    # Iterate over each element in list\n",
    "    for elem in listOfElems:\n",
    "        # If element exists in dict then increment its value else add it in dict\n",
    "        if elem in dictOfElems:\n",
    "            dictOfElems[elem] += 1\n",
    "        else:\n",
    "            dictOfElems[elem] = 1    \n",
    " \n",
    "    # Filter key-value pairs in dictionary. Keep pairs whose value is greater than 1 i.e. only duplicate elements from list.\n",
    "    dictOfElems = { key:value for key, value in dictOfElems.items()} #  if value > 1\n",
    "    # Returns a dict of duplicate elements and thier frequency count\n",
    "    return dictOfElems\n",
    "\n",
    "def class_ents(text):\n",
    "    classify = \"\"\n",
    "    if text.ents:\n",
    "        for ent in text.ents:\n",
    "            classify = ent.label_\n",
    "    return classify\n",
    "\n",
    "def tag_user(screen_name, profile_name):\n",
    "    c = \"UND\"\n",
    "    if re.findall(regex, screen_name):\n",
    "        screen_name = re.sub(r\"_\", \" \", str(screen_name))\n",
    "    screen_name = screen_name.strip()\n",
    "    screen_name = screen_name[0].upper() + screen_name[1:]\n",
    "    c1 = class_ents(nlp(profile_name))\n",
    "    c2 = class_ents(nlp(screen_name))\n",
    "    if c1 == c2 and c1 != \"\":\n",
    "        c = c1\n",
    "    else:\n",
    "        if c2:\n",
    "            c = c2\n",
    "    if c1 == \"PER\" or c2 == \"PER\" or c1 == \"LOC\" or c2 == \"LOC\":\n",
    "        c = \"PER\"\n",
    "    return c\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN']):\n",
    "    texts_out = [ token.text for token in nlp(texts) if token.pos_ in \n",
    "                 allowed_postags and token.text not in black_list and len(token.text)>2]\n",
    "    return texts_out\n",
    "\n",
    "def display_topics(model, model_type=\"lda\"):\n",
    "  for topic_idx, topic in enumerate(model.print_topics()):\n",
    "    print (\"Topic %d:\" % (topic_idx))\n",
    "    if model_type== \"hdp\":\n",
    "      print (\" \".join(re.findall( r'\\*(.[^\\*-S]+).?', topic[1])), \"\\n\")\n",
    "    else:\n",
    "      print (\" \".join(re.findall( r'\\\"(.[^\"]+).?', topic[1])), \"\\n\")\n",
    "\n",
    "def evaluate_graph(dictionary, corpus, texts, limit, model):\n",
    "    \"\"\"\n",
    "    Function to display num_topics - LDA graph using c_v coherence\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    limit : topic limit\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lm_list : List of LDA topic models\n",
    "    c_v : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    c_v = []\n",
    "    lm_list = []\n",
    "    for num_topics in range(1, limit):\n",
    "        if model == 'lsi':\n",
    "          lm = LsiModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        else:\n",
    "          lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        lm_list.append(lm)\n",
    "        cm = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        c_v.append(cm.get_coherence())\n",
    "        \n",
    "    # Show graph\n",
    "    x = range(1, limit)\n",
    "    plt.plot(x, c_v)\n",
    "    plt.xlabel(\"num_topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"c_v\"), loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return lm_list, c_v\n",
    "\n",
    "def format_topics_sentences(n, model, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()-n\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(model[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = model.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c54e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lectura del csv a normalizar\n",
    "df = pd.read_csv(\"TWEETS_CENTRALIDAD/tweets_NB2_3_1_CENTRALIDAD.csv\")\n",
    "\n",
    "df.head()\n",
    "df_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff7d9e",
   "metadata": {},
   "source": [
    "#### verificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9da5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tratra = {0:False, 1:True}\n",
    "rev_subs = { v:k for k,v in tratra.items()}\n",
    "columna_final = [rev_subs.get(item,item) for item in df['verificado'].tolist()]\n",
    "df_final['verificado'] = columna_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91847c28",
   "metadata": {},
   "source": [
    "#### hater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc27c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_percentage = df['hate'] * 100 / df['status_retrieving']\n",
    "haters = hate_percentage > 5\n",
    "df_final['hater'] = haters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee2f00",
   "metadata": {},
   "source": [
    "#### clase NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3198a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_tagged = []\n",
    "for i, v in df.iterrows():\n",
    "    tag = tag_user(v['screen_name'], v['user_name'])\n",
    "    users_tagged.append(tag)\n",
    "rev_subs = { v:k for k,v in ner_dict.items()}\n",
    "fin = [rev_subs.get(item,item) for item in users_tagged]\n",
    "df_final['clase_NER'] = fin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ba1c3",
   "metadata": {},
   "source": [
    "#### clase DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a2405b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juanmoreno/anaconda3/envs/TFG/lib/python3.6/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['descripcion'] = df['descripcion'].fillna('')\n",
    "bigram = gensim.models.Phrases(df['descripcion'].to_list())\n",
    "\n",
    "bigram_list = bigram[df['descripcion'].to_list()]\n",
    "out_text = lemmatization(\" \".join(bigram_list))\n",
    "\n",
    "dataset = [d.split() for d in out_text]\n",
    "dictionary = Dictionary(dataset)\n",
    "dictionary.compactify()\n",
    "# Filter extremes\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.97, keep_n=None)\n",
    "dictionary.compactify()\n",
    "corpus = [dictionary.doc2bow(text) for text in dataset]\n",
    "\n",
    "hdpmodel = HdpModel(corpus=corpus, id2word=dictionary, random_state= 30)\n",
    "lsimodel = LsiModel(corpus=corpus, num_topics=10, id2word=dictionary)\n",
    "ldamodel = LdaModel(corpus=corpus, num_topics=10, id2word=dictionary)\n",
    "\n",
    "lsitopics = [[word for word, prob in topic] for topicid, topic in lsimodel.show_topics(formatted=False)]\n",
    "hdptopics = [[word for word, prob in topic] for topicid, topic in hdpmodel.show_topics(formatted=False)]\n",
    "ldatopics = [[word for word, prob in topic] for topicid, topic in ldamodel.show_topics(formatted=False)]\n",
    "\n",
    "lsi_coherence = CoherenceModel(topics=lsitopics[:10], texts=dataset, dictionary=dictionary, window_size=10).get_coherence()\n",
    "hdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=dataset, dictionary=dictionary, window_size=10).get_coherence()\n",
    "lda_coherence = CoherenceModel(topics=ldatopics, texts=dataset, dictionary=dictionary, window_size=10).get_coherence()\n",
    "\n",
    "coherences = [lsi_coherence, hdp_coherence, lda_coherence]\n",
    "n = len(coherences)\n",
    "x = ['lsi_coherence','hdp_coherence', 'lda_coherence']\n",
    "sns.barplot(x, coherences)\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(n, model=hdpmodel, corpus=corpus, texts=dataset)\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "df_dominant_topic['Dominant_Topic'] = df_dominant_topic['Dominant_Topic'].fillna(0)\n",
    "df_dominant_topic['Dominant_Topic'] = df_dominant_topic['Dominant_Topic'].astype('int64')\n",
    "\n",
    "label_dicc = {0:'opinión', 1:'estudios', 2:'política', 3: 'actividades'}\n",
    "\n",
    "opinion = df_dominant_topic['Keywords'][0] + df_dominant_topic['Keywords'][4]\n",
    "estudios = df_dominant_topic['Keywords'][1] + df_dominant_topic['Keywords'][6]\n",
    "politica = df_dominant_topic['Keywords'][2] + df_dominant_topic['Keywords'][5] + df_dominant_topic['Keywords'][8]\n",
    "actividades = df_dominant_topic['Keywords'][3] + df_dominant_topic['Keywords'][7]\n",
    "df_dominant_topic_final = pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "matriz_textos = []\n",
    "for i in [opinion, estudios, politica, actividades]:\n",
    "    list_to_str = \",\".join([i])\n",
    "    matriz_textos.append(list_to_str)\n",
    "\n",
    "for index, lista_textos in enumerate(matriz_textos):\n",
    "    df_dominant_topic_final.at[index, 'text'] = lista_textos\n",
    "\n",
    "matriz_labels = []\n",
    "for i in label_dicc.values():\n",
    "    matriz_labels.append(i)\n",
    "\n",
    "for index, lista_labels in enumerate(matriz_labels):\n",
    "    df_dominant_topic_final.at[index, 'label'] = lista_labels\n",
    "\n",
    "# df['Title'] = df['Title'].str.replace('\\(\\(\\(', '>>')\n",
    "### Clasificacion de la descripcion por temas\n",
    "for i,v in df.iterrows():\n",
    "    for j,y in df_dominant_topic_final.iterrows():\n",
    "        a_dictionary = {\"count0\": 0, \"count1\": 0, \"count2\": 0, \"count3\": 0}\n",
    "        for word in y['text'].split(', '):\n",
    "            string = word.replace('(', '')\n",
    "            \n",
    "            if re.search(string, str(v['descripcion'])) != None:\n",
    "                if j == 0:\n",
    "                    a_dictionary[\"count0\"] += 1\n",
    "                elif j == 1:\n",
    "                    a_dictionary[\"count1\"] += 1\n",
    "                elif j == 2:\n",
    "                    a_dictionary[\"count2\"] += 1\n",
    "                elif j == 3:\n",
    "                    a_dictionary[\"count3\"] += 1\n",
    "                max_key = max(a_dictionary, key=a_dictionary.get)\n",
    "                if max_key == \"count0\":\n",
    "                    df_final.at[i,'clase_DESCR'] = 0\n",
    "                elif max_key == \"count1\":\n",
    "                    df_final.at[i,'clase_DESCR'] = 1\n",
    "                elif max_key == \"count2\":\n",
    "                    df_final.at[i,'clase_DESCR'] = 2\n",
    "                elif max_key == \"count3\":\n",
    "                    df_final.at[i,'clase_DESCR'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bb5beb",
   "metadata": {},
   "source": [
    "#### clase_FECHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be2adb95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "# df = df.drop(labels=[517], axis=0)\n",
    "\n",
    "clase_fecha = []\n",
    "index = 0\n",
    "\n",
    "for item in df['fecha_creacion']:\n",
    "    datem = datetime.strptime(item[:19], \"%Y-%m-%d %H:%M:%S\")\n",
    "    anyo = datem.year\n",
    "    if int(anyo) < 2015:\n",
    "        clase_fecha.append(0)\n",
    "    elif int(anyo) >= 2015 and int(anyo) < 2019:\n",
    "        clase_fecha.append(1)\n",
    "    elif int(anyo) >= 2019:\n",
    "        clase_fecha.append(2)\n",
    "\n",
    "#print(len(clase_fecha))\n",
    "#df_final = df_final.drop(labels=[517], axis=0)\n",
    "df_final['clase_FECHA'] = clase_fecha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84fa3b5",
   "metadata": {},
   "source": [
    "#### clase_HASHTAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea07bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "hashtags = []\n",
    "x = []\n",
    "df_hashtags = pd.DataFrame(columns=['hashtags'])\n",
    "\n",
    "df['top_hashtags'] = df['top_hashtags'].fillna('{}')\n",
    "\n",
    "for content in df['top_hashtags']:\n",
    "    x.append(list(ast.literal_eval(content).keys()))\n",
    "    hashtags.extend(ast.literal_eval(content).keys())\n",
    "df_hashtags['hashtags'] = x\n",
    "\n",
    "flattened_hashtags_df = pd.DataFrame(\n",
    "    [hashtag for hashtags_list in df_hashtags.hashtags\n",
    "    for hashtag in hashtags_list],\n",
    "    columns=['hashtag'])\n",
    "\n",
    "popular_hashtags = flattened_hashtags_df.groupby('hashtag').size()\\\n",
    "                                        .reset_index(name='counts')\\\n",
    "                                        .sort_values('counts', ascending=False)\\\n",
    "                                        .reset_index(drop=True)\n",
    "\n",
    "# take hashtags which appear at least this amount of times\n",
    "min_appearance = 5\n",
    "# find popular hashtags - make into python set for efficiency\n",
    "popular_hashtags_set = set(popular_hashtags[\n",
    "                           popular_hashtags.counts>=min_appearance\n",
    "                           ]['hashtag'])\n",
    "\n",
    "# make a new column with only the popular hashtags\n",
    "df_hashtags['popular_hashtags'] = df_hashtags.hashtags.apply(\n",
    "            lambda hashtag_list: [hashtag for hashtag in hashtag_list\n",
    "                                  if hashtag in popular_hashtags_set])\n",
    "\n",
    "# drop rows without popular hashtag\n",
    "popular_hashtags_list_df = df_hashtags.loc[\n",
    "            df_hashtags.popular_hashtags.apply(lambda hashtag_list: hashtag_list !=[])]\n",
    "\n",
    "# make new dataframe\n",
    "hashtag_vector_df = popular_hashtags_list_df.loc[:, ['popular_hashtags']]\n",
    "\n",
    "for hashtag in popular_hashtags_set:\n",
    "    # make columns to encode presence of hashtags\n",
    "    hashtag_vector_df['{}'.format(hashtag)] = hashtag_vector_df.popular_hashtags.apply(\n",
    "        lambda hashtag_list: int(hashtag in hashtag_list))\n",
    "\n",
    "diccionario_manual = {\"política\" : ['Marlaska', \"Paro\", \"Ayuso\", \"Izquierda\", \"Derecha\", \"Vox\", \n",
    "                                    \"4demayo\", \"2demayo\",\"4Mayo\", \"PSOE\", \"8M\", \"4M\", \"Nazi\", \n",
    "                                    \"Elecciones\", \"Vota\", \"Reforma\", \"VotaLibertad\", \n",
    "                                    \"ProtegeMadrid\", u\"Corrupción\", \"Voto\", \"Reforma\", \n",
    "                                    \"Ley\", u\"Artículo\", u\"Abstención\", \"Abstenerse\", \n",
    "                                    \"Dictadura\", \"Libertad\", \"Comunismo\", \"Ultraderecha\", \n",
    "                                    \"Democracia\", \"Fascismo\", \"Fascista\", \"Facha\", \"Menas\", \n",
    "                                    \"Democratica\", \"Gobierno\", u\"Autonomías\", \"Aborto\", \n",
    "                                    \"Derechos\", \"Derecho\", u\"Represión\", \"PP\", u\"MásMadrid\", \n",
    "                                    \"Cs\", \"Ciudadanos\"],\n",
    "                      \"prensa\": [\"UltimaHora\", \"News\", \"AHORA\", u\"ATENCIÓN\", \"COVID\", u\"COVID-19\", \n",
    "                                 \"COVID19\", \"Colombia\", \"Cadena\", \"Pandemia\", \"Coronavirus\", \n",
    "                                 \"QuedateEnCasa\", \"YoMeQuedoEnCasa\"], \n",
    "                      \"deportes\": [\"Superliga\", \"HalaMadrid\", \"Atleti\", \"Jugones\", \"GranadaEibar\", \"Tokio2020\", \"GP\", \n",
    "                                   \"UEFA\", \"Champions\", \"Liga\", \"Futbol\", \"Baloncesto\", \"Tenis\", \"FC\", \"Levante\", \"Sevilla\",\n",
    "                                   \"Betis\", \"RayoVallecano\", \"Barça\", \"Barcelona\", \"Elche\", \"RealMadrid\", u\"Clásico\",\n",
    "                                   \"Eurocopa\", \"League\", \"Chiringuito\"]}\n",
    "\n",
    "df['top_hashtags'] = df['top_hashtags'].fillna({})\n",
    "for i,v in df.iterrows():\n",
    "    user = v.top_hashtags\n",
    "    if str(user) == '0' or str(user) == 'False':\n",
    "        df_final.at[i,'clase_HASHTAGS'] = int(0)\n",
    "    else:\n",
    "        a_dictionary = {\"count0\": 0, \"count1\": 0, \"count2\": 0, \"count3\": 0}\n",
    "        for clase in list(ast.literal_eval(user).keys())[:3]:\n",
    "            for j,y in diccionario_manual.items():\n",
    "                for word in y:\n",
    "                    if re.search(word.lower(), clase.lower()) != None:\n",
    "                        if j == \"política\":\n",
    "                            a_dictionary[\"count0\"] += 1\n",
    "                        elif j == \"prensa\":\n",
    "                            a_dictionary[\"count1\"] += 1\n",
    "                        elif j == \"deportes\":\n",
    "                            a_dictionary[\"count2\"] += 1\n",
    "                    elif re.search(word.lower(), clase.lower()) == None:\n",
    "                        a_dictionary[\"count3\"] = 1\n",
    "                max_key = max(a_dictionary, key=a_dictionary.get)\n",
    "                if max_key == \"count0\":\n",
    "                    df_final.at[i,'clase_HASHTAGS'] = int(0)\n",
    "                elif max_key == \"count1\":\n",
    "                    df_final.at[i,'clase_HASHTAGS'] = int(1)\n",
    "                elif max_key == \"count2\":\n",
    "                    df_final.at[i,'clase_HASHTAGS'] = int(2)\n",
    "                elif max_key == \"count3\": # otros\n",
    "                    df_final.at[i,'clase_HASHTAGS'] = int(3)\n",
    "                \n",
    "df_final['clase_DESCR'] = df_final['clase_DESCR'].fillna(0).astype(int)\n",
    "df_final['clase_HASHTAGS'] = df_final['clase_HASHTAGS'].fillna(0).astype(int)\n",
    "# df_final = df_final.fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4697663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verificado</th>\n",
       "      <th>hater</th>\n",
       "      <th>clase_NER</th>\n",
       "      <th>clase_DESCR</th>\n",
       "      <th>clase_FECHA</th>\n",
       "      <th>clase_HASHTAGS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4753 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      verificado  hater  clase_NER  clase_DESCR  clase_FECHA  clase_HASHTAGS\n",
       "0              0  False          0            0            2               3\n",
       "1              0  False          0            0            2               0\n",
       "2              0   True          1            0            0               0\n",
       "3              0   True          0            0            0               3\n",
       "4              0  False          0            3            0               3\n",
       "...          ...    ...        ...          ...          ...             ...\n",
       "4748           0  False          0            0            2               0\n",
       "4749           0  False          0            1            0               3\n",
       "4750           0  False          3            0            0               2\n",
       "4751           0  False          0            0            0               0\n",
       "4752           0  False          0            1            0               0\n",
       "\n",
       "[4753 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076a3e7",
   "metadata": {},
   "source": [
    "#### clase_CATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc374c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'españa', 1: 'cultura', 2: 'sociedad', 3: 'arte', 4: 'viñetas', 5: 'cataluña', 6: 'artes gráficas', 7: 'dibujo', 8: 'opinión', 9: 'ilustración', 10: 'política', 11: 'otros'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verificado</th>\n",
       "      <th>hater</th>\n",
       "      <th>clase_NER</th>\n",
       "      <th>clase_DESCR</th>\n",
       "      <th>clase_FECHA</th>\n",
       "      <th>clase_HASHTAGS</th>\n",
       "      <th>clase_CATEGORIAS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4653 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      verificado  hater  clase_NER  clase_DESCR  clase_FECHA  clase_HASHTAGS  \\\n",
       "100            0  False          1            2            0               3   \n",
       "101            0   True          3            0            1               3   \n",
       "102            0  False          0            0            2               3   \n",
       "103            0   True          0            0            2               0   \n",
       "104            0  False          0            0            1               3   \n",
       "...          ...    ...        ...          ...          ...             ...   \n",
       "4748           0  False          0            0            2               0   \n",
       "4749           0  False          0            1            0               3   \n",
       "4750           0  False          3            0            0               2   \n",
       "4751           0  False          0            0            0               0   \n",
       "4752           0  False          0            1            0               0   \n",
       "\n",
       "      clase_CATEGORIAS  \n",
       "100                0.0  \n",
       "101                0.0  \n",
       "102                0.0  \n",
       "103                0.0  \n",
       "104                1.0  \n",
       "...                ...  \n",
       "4748               0.0  \n",
       "4749              11.0  \n",
       "4750               0.0  \n",
       "4751               1.0  \n",
       "4752               0.0  \n",
       "\n",
       "[4653 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_categorias = []\n",
    "\n",
    "df['top_categories'] = df['top_categories'].fillna(\"{}\")\n",
    "\n",
    "for user in df.top_categories:\n",
    "    lista_categorias.extend(ast.literal_eval(user))\n",
    "    \n",
    "dictOfElems = getDuplicatesWithCount(lista_categorias)\n",
    "diccionario_categorias = {}\n",
    "for key, value in dictOfElems.items():\n",
    "    diccionario_categorias.update({key:value})\n",
    "\n",
    "n = 11\n",
    "topitems = heapq.nlargest(n, diccionario_categorias.items(), key=operator.itemgetter(1))\n",
    "topitemsasdict = dict(topitems)\n",
    "\n",
    "diccionario_clases_fin = {x:y[0] for x, y in enumerate(topitems)}\n",
    "diccionario_clases_fin.update({11:'otros'})\n",
    "print(diccionario_clases_fin)\n",
    "\n",
    "diccionario_manual = {0: 'españa', 1: 'cultura', 2: 'sociedad', 3: 'arte', 4: 'viñetas', 5: 'cataluña',\n",
    "                      6: 'artes gráficas', 7: 'dibujo', 8: 'opinión', 9: 'ilustración', 10: 'política', 11: 'otros'}\n",
    "\n",
    "for i, v in df.iterrows():\n",
    "    user = v.top_categories\n",
    "    a_dictionary = {\"count0\": 0, \"count1\": 0, \"count2\": 0, \"count3\": 0, \"count4\": 0, \"count5\": 0, \"count6\": 0, \"count7\": 0, \"count8\": 0, \"count9\": 0, \"count10\": 0, \"count11\": 0}\n",
    "    \n",
    "    if len(list(ast.literal_eval(user).keys())) == 0:\n",
    "        df_final.at[i,'clase_CATEGORIAS'] = int(0)\n",
    "    else:\n",
    "    \n",
    "        clase = list(ast.literal_eval(user).keys())[0]\n",
    "        for j, y in diccionario_manual.items():\n",
    "            if y == clase:\n",
    "                if y == \"españa\":\n",
    "                    a_dictionary[\"count0\"] += 1\n",
    "                elif y == \"cultura\":\n",
    "                    a_dictionary[\"count1\"] += 1\n",
    "                elif y == \"sociedad\":\n",
    "                    a_dictionary[\"count2\"] += 1\n",
    "                elif y == \"arte\":\n",
    "                    a_dictionary[\"count3\"] += 1\n",
    "                elif y == \"viñetas\":\n",
    "                    a_dictionary[\"count4\"] += 1\n",
    "                elif y == \"cataluña\":\n",
    "                    a_dictionary[\"count5\"] += 1\n",
    "                elif y == \"artes gráficas\":\n",
    "                    a_dictionary[\"count6\"] += 1\n",
    "                elif y == \"dibujo\":\n",
    "                    a_dictionary[\"count7\"] += 1\n",
    "                elif y == \"opinión\":\n",
    "                    a_dictionary[\"count8\"] += 1\n",
    "                elif y == \"ilustración\":\n",
    "                    a_dictionary[\"count9\"] += 1\n",
    "                elif y == \"literatura\":\n",
    "                    a_dictionary[\"count10\"] += 1\n",
    "            else:\n",
    "                a_dictionary[\"count11\"] = 1 #otros\n",
    "            max_key = max(a_dictionary, key=a_dictionary.get)\n",
    "            if max_key == \"count0\":\n",
    "                df_final.at[i,'clase_CATEGORIAS'] = int(0)\n",
    "            elif max_key == \"count1\":\n",
    "                df_final.at[i,'clase_CATEGORIAS'] = int(1)\n",
    "            elif max_key == \"count2\":\n",
    "                df_final.at[i,'clase_CATEGORIAS'] = int(2)\n",
    "            elif max_key == \"count3\":\n",
    "                df_final.at[i,'clase_CATEGORIAS'] = int(3)\n",
    "            elif max_key == \"count4\":\n",
    "                df_final.at[i,'clase_CATEGORIAS'] = int(4)\n",
    "            elif max_key == \"count5\":\n",
    "                df_final.at[i,'clase_CATEGORIAS'] = int(5)\n",
    "            elif max_key == \"count6\": # otros\n",
    "                df_final.at[i,'clase_CATEGORIAS'] = int(6)\n",
    "            elif max_key == \"count7\":\n",
    "                df_final.at[i,'clase_CATEGORIAS'] = int(7)\n",
    "            elif max_key == \"count8\":\n",
    "                df_final.at[i,'clase_CATEGORIAS'] = int(8)\n",
    "            elif max_key == \"count9\":\n",
    "                df_final.at[i,'clase_CATEGORIAS'] = int(9)\n",
    "            elif max_key == \"count10\":\n",
    "                df_final.at[i,'clase_CATEGORIAS'] = int(10)\n",
    "            elif max_key == \"count11\":\n",
    "                df_final.at[i,'clase_CATEGORIAS'] = int(11)\n",
    "df_final[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2504a764",
   "metadata": {},
   "source": [
    "#### clase_DOMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad478d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verificado</th>\n",
       "      <th>hater</th>\n",
       "      <th>clase_NER</th>\n",
       "      <th>clase_DESCR</th>\n",
       "      <th>clase_FECHA</th>\n",
       "      <th>clase_HASHTAGS</th>\n",
       "      <th>clase_CATEGORIAS</th>\n",
       "      <th>clase_DOMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4753 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      verificado  hater  clase_NER  clase_DESCR  clase_FECHA  clase_HASHTAGS  \\\n",
       "0              0  False          0            0            2               3   \n",
       "1              0  False          0            0            2               0   \n",
       "2              0   True          1            0            0               0   \n",
       "3              0   True          0            0            0               3   \n",
       "4              0  False          0            3            0               3   \n",
       "...          ...    ...        ...          ...          ...             ...   \n",
       "4748           0  False          0            0            2               0   \n",
       "4749           0  False          0            1            0               3   \n",
       "4750           0  False          3            0            0               2   \n",
       "4751           0  False          0            0            0               0   \n",
       "4752           0  False          0            1            0               0   \n",
       "\n",
       "      clase_CATEGORIAS  clase_DOMS  \n",
       "0                 11.0         0.0  \n",
       "1                  0.0         0.0  \n",
       "2                 11.0         0.0  \n",
       "3                  0.0         1.0  \n",
       "4                  0.0         0.0  \n",
       "...                ...         ...  \n",
       "4748               0.0         0.0  \n",
       "4749              11.0         0.0  \n",
       "4750               0.0         0.0  \n",
       "4751               1.0         0.0  \n",
       "4752               0.0         0.0  \n",
       "\n",
       "[4753 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicc_topic_domains = {}\n",
    "dicc_topic_domains['www.elcatalan.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.lavanguardia.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.europapress.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['politico.ec'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.theguardian.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['ver.20m.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.lanacion.com.ar'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.laprensa.com.ar'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['lavoz.gal'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.eluniversal.com.mx'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['abcnoticias.mx'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.bbc.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.eldiariomontanes.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['vozlibre.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.elmundo.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['ver.abc.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.informationliberation.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.vozpopuli.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.elespanol.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.elconfidencial.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.abc.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['altavozdesucesos.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['elEconomista.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['laultimahora.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.rtve.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.elfinanciero.com.mx'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.eldiario.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.elheraldo.co'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.telediario.mx'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.eleconomista.com.mx'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.diariodesevilla.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.diariosur.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.elciudadano.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.publico.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.diariodeleon.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.diarioregistrado.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['coordenadainformativa.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.larazon.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.elliberal.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.elnacional.cat'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.ultimahora.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.elliberal.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.lanacion.com.py'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.naciodigital.cat'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['DIARIO.ES'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['periodismoypunto.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.diariovasco.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.elnacional.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.washingtonpost.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.elperiodico.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.france24.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.elnacional.com'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.diariodemallorca.es'] = 'información, comunicación y actualidad'\n",
    "dicc_topic_domains['www.facebook.com'] = 'redes sociales'\n",
    "dicc_topic_domains['m.facebook.com'] = 'redes sociales'\n",
    "\n",
    "for key, value in dicc_topic_domains.items():\n",
    "    if value == \"redes sociales\":\n",
    "        dicc_topic_domains[key] = int(0)\n",
    "    elif value == \"información, comunicación y actualidad\":\n",
    "        dicc_topic_domains[key] = int(1)\n",
    "    elif value == \"entretenimiento\":\n",
    "        dicc_topic_domains[key] = int(2)\n",
    "\n",
    "\n",
    "aux_dicc = {}\n",
    "\n",
    "\"\"\"dict_nan = {}\n",
    "dict_nan[\"a\"] = 1\n",
    "\n",
    "df['top_referenced_domains'] = df['top_referenced_domains'].fillna(dict_nan)\"\"\"\n",
    "\n",
    "\n",
    "for i, doms in enumerate(df.top_referenced_domains):\n",
    "    if str(doms) == 'nan':\n",
    "        print(i)\n",
    "        print(\"nan\")\n",
    "        doms = [\"a\"]\n",
    "    else:\n",
    "        doms = ast.literal_eval(doms)\n",
    "        lista_doms = []\n",
    "        for dom in doms:\n",
    "            if dom in dicc_topic_domains.keys():\n",
    "                lista_doms.append(dicc_topic_domains[dom])\n",
    "        aux_dicc.update({i:lista_doms})\n",
    "        indice = np.argmax([lista_doms.count(0),lista_doms.count(1),lista_doms.count(2)])\n",
    "        df_final.at[i,'clase_DOMS'] = int(indice)\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4491a79",
   "metadata": {},
   "source": [
    "#### Negativos, positivos, neutros, n_hate, n_nohate, baddies y scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81fcec0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verificado</th>\n",
       "      <th>hater</th>\n",
       "      <th>clase_NER</th>\n",
       "      <th>clase_DESCR</th>\n",
       "      <th>clase_FECHA</th>\n",
       "      <th>clase_HASHTAGS</th>\n",
       "      <th>clase_CATEGORIAS</th>\n",
       "      <th>clase_DOMS</th>\n",
       "      <th>negativos</th>\n",
       "      <th>positivos</th>\n",
       "      <th>neutros</th>\n",
       "      <th>n_hate</th>\n",
       "      <th>n_nohate</th>\n",
       "      <th>n_baddies</th>\n",
       "      <th>negativos_score</th>\n",
       "      <th>positivos_score</th>\n",
       "      <th>neutros_score</th>\n",
       "      <th>hate_score</th>\n",
       "      <th>no_hate_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>34.5</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.961180</td>\n",
       "      <td>0.944002</td>\n",
       "      <td>0.910685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.933036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>99.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.945017</td>\n",
       "      <td>0.954864</td>\n",
       "      <td>0.925253</td>\n",
       "      <td>0.999228</td>\n",
       "      <td>0.936098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>74.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.961832</td>\n",
       "      <td>0.938198</td>\n",
       "      <td>0.901452</td>\n",
       "      <td>0.995762</td>\n",
       "      <td>0.910090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.952856</td>\n",
       "      <td>0.906164</td>\n",
       "      <td>0.929116</td>\n",
       "      <td>0.887177</td>\n",
       "      <td>0.942912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.951490</td>\n",
       "      <td>0.891405</td>\n",
       "      <td>0.915006</td>\n",
       "      <td>0.899269</td>\n",
       "      <td>0.932453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>48.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>95.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.968723</td>\n",
       "      <td>0.916508</td>\n",
       "      <td>0.928989</td>\n",
       "      <td>0.998813</td>\n",
       "      <td>0.940292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>48.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>98.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.924751</td>\n",
       "      <td>0.928762</td>\n",
       "      <td>0.947140</td>\n",
       "      <td>0.989695</td>\n",
       "      <td>0.935377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.922024</td>\n",
       "      <td>0.868029</td>\n",
       "      <td>0.920553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.912080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.962710</td>\n",
       "      <td>0.960138</td>\n",
       "      <td>0.926720</td>\n",
       "      <td>0.997943</td>\n",
       "      <td>0.947189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>95.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.970421</td>\n",
       "      <td>0.927314</td>\n",
       "      <td>0.933247</td>\n",
       "      <td>0.987658</td>\n",
       "      <td>0.941383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4753 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      verificado  hater  clase_NER  clase_DESCR  clase_FECHA  clase_HASHTAGS  \\\n",
       "0              0  False          0            0            2               3   \n",
       "1              0  False          0            0            2               0   \n",
       "2              0   True          1            0            0               0   \n",
       "3              0   True          0            0            0               3   \n",
       "4              0  False          0            3            0               3   \n",
       "...          ...    ...        ...          ...          ...             ...   \n",
       "4748           0  False          0            0            2               0   \n",
       "4749           0  False          0            1            0               3   \n",
       "4750           0  False          3            0            0               2   \n",
       "4751           0  False          0            0            0               0   \n",
       "4752           0  False          0            1            0               0   \n",
       "\n",
       "      clase_CATEGORIAS  clase_DOMS  negativos  positivos  neutros  n_hate  \\\n",
       "0                 11.0         0.0       21.5       34.5     44.0     0.0   \n",
       "1                  0.0         0.0       31.0       17.0     52.0     0.5   \n",
       "2                 11.0         0.0       21.5        4.5     74.0     7.0   \n",
       "3                  0.0         1.0       49.0       10.0     41.0     8.0   \n",
       "4                  0.0         0.0       51.5        8.5     40.0     2.0   \n",
       "...                ...         ...        ...        ...      ...     ...   \n",
       "4748               0.0         0.0       39.0       12.5     48.5     4.5   \n",
       "4749              11.0         0.0       37.0       14.5     48.5     1.5   \n",
       "4750               0.0         0.0       31.0       17.0     52.0     0.0   \n",
       "4751               1.0         0.0       29.5       31.0     39.5     1.0   \n",
       "4752               0.0         0.0       31.0       22.0     47.0     4.5   \n",
       "\n",
       "      n_nohate  n_baddies  negativos_score  positivos_score  neutros_score  \\\n",
       "0        100.0        7.5         0.961180         0.944002       0.910685   \n",
       "1         99.5        6.0         0.945017         0.954864       0.925253   \n",
       "2         93.0        7.0         0.961832         0.938198       0.901452   \n",
       "3         92.0       14.5         0.952856         0.906164       0.929116   \n",
       "4         98.0       17.5         0.951490         0.891405       0.915006   \n",
       "...        ...        ...              ...              ...            ...   \n",
       "4748      95.5       16.0         0.968723         0.916508       0.928989   \n",
       "4749      98.5        8.5         0.924751         0.928762       0.947140   \n",
       "4750     100.0       11.0         0.922024         0.868029       0.920553   \n",
       "4751      99.0        9.0         0.962710         0.960138       0.926720   \n",
       "4752      95.5        4.5         0.970421         0.927314       0.933247   \n",
       "\n",
       "      hate_score  no_hate_score  \n",
       "0       0.000000       0.933036  \n",
       "1       0.999228       0.936098  \n",
       "2       0.995762       0.910090  \n",
       "3       0.887177       0.942912  \n",
       "4       0.899269       0.932453  \n",
       "...          ...            ...  \n",
       "4748    0.998813       0.940292  \n",
       "4749    0.989695       0.935377  \n",
       "4750    0.000000       0.912080  \n",
       "4751    0.997943       0.947189  \n",
       "4752    0.987658       0.941383  \n",
       "\n",
       "[4753 rows x 19 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, row in df.iterrows():\n",
    "    negativos = row['negativos'] * 100 / row['status_retrieving']\n",
    "    positivos = row['positivos'] * 100 / row['status_retrieving']\n",
    "    neutros = row['neutros'] * 100 / row['status_retrieving']\n",
    "    n_hate = row['hate'] * 100 / row['status_retrieving']\n",
    "    n_nohate = row['no_hate'] * 100 / row['status_retrieving']\n",
    "    n_baddies = row['n_baddies'] * 100 / row['status_retrieving']\n",
    "    \n",
    "    df_final.at[i,'negativos'] = negativos\n",
    "    df_final.at[i,'positivos'] = positivos\n",
    "    df_final.at[i,'neutros'] = neutros\n",
    "    df_final.at[i,'n_hate'] = n_hate\n",
    "    df_final.at[i,'n_nohate'] = n_nohate\n",
    "    df_final.at[i,'n_baddies'] = n_baddies\n",
    "    \n",
    "    df_final.at[i,'negativos_score'] = row['negativos_score']\n",
    "    df_final.at[i,'positivos_score'] = row['positivos_score']\n",
    "    df_final.at[i,'neutros_score'] = row['neutros_score']\n",
    "    df_final.at[i,'hate_score'] = row['hate_score']\n",
    "    df_final.at[i,'no_hate_score'] = row['no_hate_score']\n",
    "    \n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4417a97b",
   "metadata": {},
   "source": [
    "#### Medidas de centralidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7896873",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    df_final.at[i,'eigenvector'] = row['eigenvector']\n",
    "    df_final.at[i,'in_degree'] = row['in_degree']\n",
    "    df_final.at[i,'out_degree'] = row['out_degree']\n",
    "    df_final.at[i,'degree'] = row['degree']\n",
    "    df_final.at[i,'clustering'] = row['clustering']\n",
    "    df_final.at[i,'closeness'] = row['closeness']\n",
    "    df_final.at[i,'betweenness'] = row['betweenness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e7af32",
   "metadata": {},
   "source": [
    "#### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd8b7495",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm = df[['statuses_count', 'num_hashtags', 'rt_count', 'num_mentions', 'num_urls', 'len_status',\n",
    "                 'times_user_quotes', 'num_rts_to_tweets', 'num_favs_to_tweets', 'misspelling_counter',\n",
    "                 'leet_counter', 'status_average_tweets_per_day']]\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit(train_norm)\n",
    "x_train_norm = std_scale.transform(train_norm)\n",
    "training_norm_col = pd.DataFrame(x_train_norm, index=train_norm.index, columns=train_norm.columns)\n",
    "\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    df_final.at[i,'statuses_count'] = training_norm_col.at[i,'statuses_count']\n",
    "    df_final.at[i,'num_hashtags'] = training_norm_col.at[i,'num_hashtags']\n",
    "    df_final.at[i,'rt_count'] = training_norm_col.at[i,'rt_count']\n",
    "    df_final.at[i,'num_mentions'] = training_norm_col.at[i,'num_mentions']\n",
    "    df_final.at[i,'num_urls'] = training_norm_col.at[i,'num_urls']\n",
    "    df_final.at[i,'len_status'] = training_norm_col.at[i,'len_status']\n",
    "    df_final.at[i,'times_user_quotes'] = training_norm_col.at[i,'times_user_quotes']\n",
    "    df_final.at[i,'num_rts_to_tweets'] = training_norm_col.at[i,'num_rts_to_tweets']\n",
    "    df_final.at[i,'num_favs_to_tweets'] = training_norm_col.at[i,'num_favs_to_tweets']\n",
    "    df_final.at[i,'misspelling_counter'] = training_norm_col.at[i,'misspelling_counter']\n",
    "    df_final.at[i,'leet_counter'] = training_norm_col.at[i,'leet_counter']\n",
    "    df_final.at[i,'status_average_tweets_per_day'] = training_norm_col.at[i,'status_average_tweets_per_day']\n",
    "    \n",
    "df_final['text'] = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d862d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"TWEETS_CENTRALIDAD_NORMALIZADOS/tweets_NB2_3_1_CENTRALIDAD_NORMALIZADO.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6df971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
